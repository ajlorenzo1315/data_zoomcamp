{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d9e6d7",
   "metadata": {},
   "source": [
    "### Importamos pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be5d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7513e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8e55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"yellow_tripdata_2021-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa35324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('yellow_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c1a83",
   "metadata": {},
   "source": [
    "Este código carga un archivo Parquet llamado \"yellow_tripdata_2021-01.csv\" en un objeto pandas DataFrame de manera iterativa en bloques de tamaño 100,000. El parámetro \"iterator=True\" especifica que el objeto DataFrame debe ser iterado en lugar de cargado por completo en memoria. Esto es útil cuando se trabaja con archivos de gran tamaño que no caben en la memoria RAM disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385a43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter =  pd.read_csv(\"yellow_tripdata_2021-01.csv\",iterator = True ,chunksize= 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57a65f",
   "metadata": {},
   "source": [
    "Bucle infinito para intrpducir chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a75541cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7f0712e11040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine=create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b04023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunck(df_new):\n",
    "    df_new.tpep_pickup_datetime=pd.to_datetime(df_new.tpep_pickup_datetime)\n",
    "    df_new.tpep_dropoff_datetime=pd.to_datetime(df_new.tpep_dropoff_datetime)\n",
    "    df_new.to_sql(name='yellow_taxi_data_sql',con=engine , if_exists='append') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14ff00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.25 s, sys: 39.6 ms, total: 3.29 s\n",
      "Wall time: 5.69 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.25 s, sys: 38.9 ms, total: 3.29 s\n",
      "Wall time: 5.67 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.34 s, sys: 19.7 ms, total: 3.36 s\n",
      "Wall time: 5.78 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.36 s, sys: 24.2 ms, total: 3.38 s\n",
      "Wall time: 5.79 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.44 s, sys: 28.4 ms, total: 3.47 s\n",
      "Wall time: 5.97 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.31 s, sys: 28.1 ms, total: 3.33 s\n",
      "Wall time: 5.7 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.35 s, sys: 35.7 ms, total: 3.39 s\n",
      "Wall time: 5.73 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.85 s, sys: 36 ms, total: 3.89 s\n",
      "Wall time: 6.73 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.77 s, sys: 35.2 ms, total: 3.8 s\n",
      "Wall time: 6.5 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.6 s, sys: 36.4 ms, total: 3.64 s\n",
      "Wall time: 6.23 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.78 s, sys: 24.5 ms, total: 3.8 s\n",
      "Wall time: 6.57 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 3.72 s, sys: 15.5 ms, total: 3.74 s\n",
      "Wall time: 6.38 s\n",
      "Inserted another chunk ..., toma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113094/1734377891.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_new=next(df_inter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.71 s, sys: 39.2 ms, total: 3.75 s\n",
      "Wall time: 6.36 s\n",
      "Inserted another chunk ..., toma\n",
      "CPU times: user 2.43 s, sys: 19.7 ms, total: 2.45 s\n",
      "Wall time: 4.14 s\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     df_new\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_inter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInserted another chunk ..., toma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunck(df_new)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1698\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1698\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1810\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:833\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    df_new=next(df_inter)\n",
    "    print(\"Inserted another chunk ..., toma\")\n",
    "    %time chunck(df_new)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc424c",
   "metadata": {},
   "source": [
    "<pre>\n",
    "root@localhost:ny_taxi> select COUNT(1) from yellow_taxi_data_sql\n",
    "+---------+\n",
    "| count   |\n",
    "|---------|\n",
    "| 1469769 |\n",
    "+---------+\n",
    "SELECT 1\n",
    "Time: 0.067s\n",
    "root@localhost:ny_taxi>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d0e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
